{"cells":[{"cell_type":"markdown","metadata":{"id":"b7EStZqliOom"},"source":["**BERT BASE CASE**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19471,"status":"ok","timestamp":1647208265471,"user":{"displayName":"martijn korf","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08663037803752538794"},"user_tz":-60},"id":"zUnWQLk_NQlv","outputId":"f4189059-9f46-4f83-8cf3-16df87e4679b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#Mount Drive for remote data access\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"eu6ce1MNM_g6"},"source":["# **Install packages**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ps4-gx1jMce9"},"outputs":[],"source":["!pip install transformers\n","!pip install tokenizers\n","!pip install tensorflow\n","!pip install tensorflow_addons\n","!pip install numba"]},{"cell_type":"markdown","metadata":{"id":"wZWH6TUGM6rB"},"source":["# **Import packages**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MhPiWNheNaw1"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.python.client import device_lib\n","import matplotlib.pyplot as plt\n","import warnings\n","from tokenizers.processors import TemplateProcessing\n","from tokenizers.pre_tokenizers import WhitespaceSplit\n","from tokenizers.models import BPE\n","from tokenizers import Tokenizer\n","from tokenizers.trainers import BpeTrainer\n","from transformers import PreTrainedTokenizerFast, BertForPreTraining\n","from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n","from transformers import pipeline, set_seed, GPT2Model, GPT2Config, GPT2Tokenizer, TFGPT2LMHeadModel, TFBertForSequenceClassification, AdamW, BertConfig, TFBertForMaskedLM\n","from sklearn.model_selection import train_test_split, ShuffleSplit, KFold\n","from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n","import logging \n","logging.basicConfig(level=logging.ERROR)\n","import random\n","from math import *\n","from tqdm import tqdm, notebook\n","import pandas as pd\n","import numpy as np\n","import os\n","import json"]},{"cell_type":"markdown","metadata":{"id":"CtgDA4KBNelG"},"source":["# **Import and view data**"]},{"cell_type":"markdown","source":["connect to GPU if available\n"],"metadata":{"id":"8B7al-PFJynB"}},{"cell_type":"code","source":["# If there's a GPU available...\n","if tf.config.list_physical_devices('GPU'):    \n","    # Tell PyTorch to use the GPU.    \n","    device = tf.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % len(tf.config.list_physical_devices('GPU')))\n","    print('We will use the GPU:', device_lib.list_local_devices()[1].name)\n","    device_name = device_lib.list_local_devices()[1].name\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = tf.device(\"cpu\")\n","    device_name = device_lib.list_local_devices()[0].name"],"metadata":{"id":"WR1uQw6vJxsb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fOXTK1cli59P"},"source":["Importing and Showing Data"]},{"cell_type":"code","source":["path_names = { 'train_data': \"/content/drive/MyDrive/Seminar QM/Data Cleaned/data_train_cln.txt\", \n","    'test_data': \"/content/drive/MyDrive/Seminar QM/Data/data_test.txt\",\n","    'tokenizer_file' : \"/content/drive/MyDrive/Seminar QM/Models/TF/Tokenizer/hamsbertje_wordlevel_cln.json\",\n","    'BERT_pretrained_model' : \"/content/drive/MyDrive/Seminar QM/Models/TF/220306 BERT pre-trained AH (REDDING)/BERT pre-trained\",\n","    'results_preck': \"results_precK_BERTbase\",\n","    'results_hyperparams': \"results_best-params_BERTbase\",\n","    'taxonomy': \"/content/drive/MyDrive/Seminar QM/Data/product_taxonomy_paths.csv\",\n","    'productID_toName': \"/content/drive/MyDrive/Seminar QM/Data/productIDs_names.txt\"\n","}\n","\n","params = {\n","    'max_tokenized_length' : 60,\n","    'batch_size' : 24,\n","    'n_seqs' : 6,\n","    'ngram_sim' : 0.8\n","}"],"metadata":{"id":"5tKx19cXKAg3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T5Rzu-K3NhFY"},"outputs":[],"source":["df_test = pd.read_csv(path_names[test_data], names=[\"prod_series\"], dtype=str)\n","baskets_test = df_test['prod_series'].to_list()\n","\n","with open(path_names['productID_toName']) as f:\n","    dict_textfile = f.read()     \n","productIDs_names = json.loads(dict_textfile)\n","\n","RANDOM_STATE = 123\n","random.seed(RANDOM_STATE)\n","tf.random.set_seed(RANDOM_STATE)"]},{"cell_type":"code","source":["# create baskets for sequential metrics\n","basket_trial = []\n","for basket in df_test['prod_series']:\n","  step = []\n","  products = len(basket.split())\n","  for i in range(products):\n","    if i>0:\n","      basket_trial.append(basket.split()[0:i+1])\n","\n","baskets_trial = []\n","for i in basket_trial:\n","  step = \"\"\n","  for j in i:\n","    step = step +\" \" +  str(j)\n","  baskets_trial.append(step)\n","\n","df_trial = pd.DataFrame(baskets_trial)"],"metadata":{"id":"4DwZHFbrncNY"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1033,"status":"ok","timestamp":1647208331722,"user":{"displayName":"martijn korf","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08663037803752538794"},"user_tz":-60},"id":"1Vw4ypSu0dEM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b1efee26-1795-4b29-a7ce-797e45c749a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["26722\n"]}],"source":["# loading tokenizer from the saved model path ; take old tokenizer\n","tokenizer = PreTrainedTokenizerFast(\n","    tokenizer_file=path_names['tokenizer_file'], # You can load from the tokenizer file, alternatively\n","    eos_token= \"</s>\",\n","    bos_token= \"<s>\",\n","    unk_token= \"<unk>\",\n","    pad_token= \"<pad>\",\n","    mask_token= \"<mask>\",\n","    padding_side = \"left\",\n","    truncation = \"left\",\n",")"]},{"cell_type":"markdown","metadata":{"id":"6VNENEyxYu-G"},"source":["# ***GPT BASE CASE***"]},{"cell_type":"markdown","metadata":{"id":"Lekt3h4f97mp"},"source":["Initialize pre-trained models"]},{"cell_type":"code","source":["global dataset_test\n","baskets_dataset = baskets_test # replace with baskets_trails for sequential data\n","dataset_test = tokenizer(baskets_test, return_tensors='tf',\n","               max_length=60, truncation=True, padding='max_length')"],"metadata":{"id":"cri4r_3K7jkC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Append labels, masking, missing items\n","#Test\n","indices_to_mask_test = np.where(dataset_test.input_ids == 2)[1]-1\n","\n","labels_tensors_test = []\n","missing_items_test = []\n","mask_tensors_test = []\n","for i in tqdm(range(tf.shape(dataset_test.input_ids)[0].numpy())):\n","  #labels\n","  labels_array_test = np.full(tf.shape(dataset_test.input_ids)[1].numpy(), -100, dtype=np.int32)\n","  labels_array_test[indices_to_mask_test[i]] = dataset_test.input_ids[i,indices_to_mask_test[i]]\n","  labels_tensors_test.append(labels_array_test)\n","  #missing items\n","  missing_items_test.append([dataset_test.input_ids[i,indices_to_mask_test[i]]])\n","  #masking\n","  bool_array_test = np.zeros(tf.shape(dataset_test.input_ids)[1].numpy(),dtype=bool)\n","  bool_array_test[indices_to_mask_test[i]] = True\n","  mask_tensors_test.append(bool_array_test)\n","\n","labels_arr_test = tf.convert_to_tensor(labels_tensors_test)\n","missing_item_arr_test = tf.convert_to_tensor(missing_items_test)\n","mask_arr_test = tf.convert_to_tensor(mask_tensors_test)\n","\n","dataset_test['labels'] = labels_arr_test\n","dataset_test['missing_items'] = missing_item_arr_test\n","dataset_test['input_ids'] = tf.where(~mask_arr_test, dataset_test.input_ids, 4)\n","dataset_test['masked_index'] = tf.convert_to_tensor(np.where(dataset_test['labels'] != -100)[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5IJuPYNL7q_G","executionInfo":{"status":"ok","timestamp":1647208396746,"user_tz":-60,"elapsed":56739,"user":{"displayName":"martijn korf","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08663037803752538794"}},"outputId":"e3bbca81-f7fb-4042-b047-bc07784af9e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 24999/24999 [00:56<00:00, 446.24it/s] \n"]}]},{"cell_type":"code","source":["global bad_words\n","bad_words = [[tokenizer.eos_token_id],[tokenizer.bos_token_id], [tokenizer.pad_token_id]]\n","\n","BERT = TFBertForMaskedLM.from_pretrained(path_names['BERT_pretrained_model'])"],"metadata":{"id":"CEQd1fde7zJN"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2VYLGkZVOanh"},"outputs":[],"source":["#returns nothing when product name not found\n","def getProductNames(A):\n","  if (tf.rank(A) <= 1): # check if rank 1, otherwise tokenizer.decode doesn't work\n","    B = tokenizer.decode(A).split(\" \")\n","    names = [productIDs_names[B[b]] for b in range(len(B)) if B[b] in productIDs_names.keys()]\n","  else:\n","    names = [getProductNames(A[i]) for i in range(len(A))]\n","  return names\n","\n","# returns \"n.a.\" when product name not found\n","def getProductNames_Real(A):\n","  if (tf.rank(A) <= 1): # check if rank 1, otherwise tokenizer.decode doesn't work\n","    B = tokenizer.decode(A).split(\" \")\n","    names = [productIDs_names[B[b]] if B[b] in productIDs_names.keys() else \"NOT AVAILABLE\" for b in range(len(B))]\n","  else:\n","    names = [getProductNames(A[i]) for i in range(len(A))]\n","  return names"]},{"cell_type":"code","source":["def dice_coef(set1, set2):\n","  count = 0\n","  for i in set1:\n","    if i in set2:\n","      count+=1    \n","  dice =  2*count/(len(set1)+len(set2))\n","  return dice\n","\n","def ngram(product):\n","    n_gram = []\n","    for word in product.split():\n","        for j in range(len(word)-2): \n","          if len(word)>=3:   \n","              n_gram.append(word[j: j + 3])\n","    return n_gram\n","\n","def standardError(bootstraps):\n","  B = len(bootstraps)\n","  return np.sqrt(1/(B-1) * sum( np.square(bootstraps - np.mean(bootstraps))))"],"metadata":{"id":"o5JwFNTpYrJj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nBBpXBGUpYQK"},"source":["TEST"]},{"cell_type":"code","source":["loader_test = tf.data.Dataset.from_tensor_slices((dataset_test['input_ids'],\n","                                              dataset_test['attention_mask'],\n","                                              dataset_test['labels'],\n","                                              dataset_test['missing_items'],\n","                                              dataset_test['masked_index']\n","                                              )).batch(params['batch_size'],drop_remainder=True)  "],"metadata":{"id":"ZBIAp0coFLEN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#-------------------------------------------------------------------------------\n","# ~~~ TEST for bootstrapping\n","#-------------------------------------------------------------------------------\n","\n","test_size = len(baskets_dataset)\n","num_specialtok = 5\n","top_1_count,top_k_count = 0,0\n","top_1_count_adj, top_k_count_adj = 0,0\n","output = {}\n","total_batches = 0\n","pr_ls_test = []\n","prec_at = []\n","prec_at_adj = []\n","loop_test = notebook.tqdm(loader_test, position=0, leave=True, colour='yellow')\n","\n","#create top_k matrix for predictions in test set\n","top_k_all = np.zeros((test_size,80),dtype=np.int32)\n","missing_items_all = np.zeros((test_size),dtype=np.int32)\n","\n","for k, batch_test in enumerate(loop_test):\n","  batch_size = len(batch_test[0])\n","  total_batches += batch_size\n","  non_predict = tf.fill((batch_size,num_specialtok),0.0)\n","\n","  incomplete_ids_test = batch_test[0]\n","  attention_mask_test = batch_test[1]\n","  labels = batch_test[2]\n","  missing_items_test = batch_test[3]\n","  incomplete_mask_test = batch_test[4]\n","\n","  \n","  outputBERT_logits = BERT.call(incomplete_ids_test, attention_mask=attention_mask_test, labels=labels,training=False).logits \n","  logits = []\n","  for m, mask_ind in enumerate(incomplete_mask_test.numpy()):\n","    lgts = outputBERT_logits[m,mask_ind,:]\n","    logits.append(lgts)\n","  outputG = tf.convert_to_tensor(logits)[:,num_specialtok:]\n","  outputG_softmax = tf.concat((non_predict, tf.nn.softmax(outputG)), axis=-1)\n","\n","  missing_items_test = np.array([missing_items_test[i][0] for i in range(batch_size)])\n","  \n","  top_k = tf.nn.top_k(outputG_softmax, k=80, sorted=True)[1]\n","\n","  top_k_items = tf.squeeze(top_k)\n","\n","  for i in range(batch_size):\n","    top_k_all[(k*batch_size)+i,:] =  top_k_items[i]\n","    missing_items_all[(k*batch_size)+i] = missing_items_test[i]\n","\n","  # calculate (normal) prec@k\n","  for i in range(0,batch_size):\n","    top_1_count += int(top_k_items[i][0] == missing_items_test[i])\n","    for j in range(params['n_seqs']):\n","      if (top_k_items[i][j] == missing_items_test[i]):\n","        top_k_count += 1\n","        prec_at.append(j)\n","        break\n","\n","  # calculate adj. prec@k\n","  top_k_names = getProductNames(top_k_items)\n","  real_names = getProductNames_Real(missing_items_test)\n","  for i in range(batch_size):\n","    real_name = real_names[i]\n","    top_k_item = top_k_names[i]\n","    if real_name == \"NOT AVAILABLE\":\n","      real_token = missing_items_test[i]\n","      top_k_token = top_k_items[i]\n","      if real_token == top_k_token[0]:\n","        top_1_count_adj +=1\n","      for j in range(params['n_seqs']):\n","        if real_token == top_k_token[j]:\n","          prec_at_adj.append(j)\n","          top_k_count_adj += 1\n","          break\n","    else:\n","      predict_names, ngrams = [], dict()\n","      q, t = 0,0\n","      ngram_correct = ngram(real_name)\n","      #print(ngram_correct)\n","      short=False\n","      while len(predict_names)!=params['n_seqs'] and q!=len(top_k_item) and short!=True:\n","        if len(top_k_item)<=params['n_seqs']:\n","          short=True\n","          predict_names = top_k_item\n","          for w, item in enumerate(predict_names):\n","            ngram_item = ngram(item)\n","            ngrams.update({w : ngram_item})\n","        else:\n","          item = top_k_item[q]   \n","          ngram_item = ngram(item)\n","          if q==0:\n","            predict_names.append(item)\n","            ngrams.update({t : ngram_item})\n","            t+=1\n","          else:\n","            sim = [dice_coef(ngram_item, value) for value in ngrams.values()]\n","            if all(z<params['ngram_sim'] for z in sim):\n","              ngrams.update({t : ngram_item})\n","              predict_names.append(item)\n","              t+=1\n","\n","          q+=1\n","        \n","      for key, value in ngrams.items():\n","        ngram_sim = dice_coef(ngram_correct, value)\n","        if ngram_sim >= params['ngram_sim']:\n","          output_want = [real_name] + predict_names\n","          output[total_batches-batch_size+i] = output_want\n","          top_k_count_adj+=1\n","          prec_at_adj.append(key)\n","          if key==0:\n","            top_1_count_adj+=1\n","          break\n","  \n","  for i in range(0,batch_size):\n","    pr = tf.math.reduce_sum(tf.where(tf.math.less(outputG_softmax[i,:], outputG_softmax[i,missing_items_test[i]]), 1.0, 0.0)).numpy()/tokenizer.vocab_size\n","    pr_ls_test.append(pr)\n","  precK_test = top_k_count/((k+1)*batch_size)\n","\n","  mpr_test = np.array(pr_ls_test).sum()/((k+1)*batch_size) \n","\n","  loop_test.set_postfix(PrecisionAtK =round(precK_test,4), Correct=top_k_count,  data=\"Test\", MPR = mpr_test)\n","\n","#print metrics\n","mpr_test = np.array(pr_ls_test).sum()/test_size\n","prec1 = (top_1_count/test_size) * 100\n","prec6 = (top_k_count/test_size) * 100\n","prec1_adj = (top_1_count_adj/test_size) * 100\n","prec6_adj = ((top_k_count_adj)/test_size) * 100\n","print(f\"   Mean Percentile Rank: {mpr_test}\")\n","print(f\"     precision at 1 (%): {prec1}\")\n","print(f\"     precision at 6 (%): {prec6}\")\n","print(f\"adj. precision at 1 (%): {prec1_adj}\")\n","print(f\"adj. precision at 6 (%): {prec6_adj}\")"],"metadata":{"id":"spbU1pLq5Rde"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#plot graphs for precision at k, different k values\n","\n","prec_array = np.array(prec_at)\n","prec_array_adj = np.array(prec_at_adj)\n","\n","cum_value_adj,prec_cum_adj, cum_value, prec_cum = [],[],[],[]\n","for i in range(params['n_seqs']):\n","    value = np.count_nonzero(np.where(prec_array==i))\n","    value_adj = np.count_nonzero(np.where(prec_array_adj==i))\n","    cum_value.append(value)\n","    prec_cum.append(sum(cum_value)/test_size)\n","    cum_value_adj.append(value_adj)\n","    prec_cum_adj.append(sum(cum_value_adj)/test_size)\n","\n","plt.plot(prec_cum)\n","plt.plot(prec_cum_adj)\n","plt.xlabel(\"@k\")\n","plt.ylabel(\"Precision\")\n","plt.legend([\"Precision@k\", \"Adj. Precision@k\"])\n","plt.show()"],"metadata":{"id":"ii7K18snH6-i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#bootstrap SEs for (normal) precision@ (takes appr. 30 seconds)\n","\n","B = 200\n","np.random.seed(100)\n","prec1_bootstraps, preck_bootstraps = np.zeros(B),  np.zeros(B)\n","\n","for _,b in enumerate(tqdm(range(B),position=0, leave=True)):\n","  #create bootstrap\n","  idx_boot = np.random.choice(list(range(0,test_size)), size=test_size, replace=True)\n","  top_k_boot = [top_k_all[idx_boot[i],0:6] for i in range(test_size)]\n","  missing_items_boot = [missing_items_all[idx_boot[i]] for i in range(test_size)]\n","\n","  # calculate bootstrap stats\n","  top_1_count,top_k_count = 0,0\n","  for i in range(0,test_size):\n","    top_1_count += int(top_k_boot[i][0] == missing_items_boot[i])\n","    for j in range(6):\n","      if (top_k_boot[i][j] == missing_items_boot[i]):\n","        top_k_count += 1\n","        break\n","  prec1_bootstraps[b] = (top_1_count / test_size)*100\n","  preck_bootstraps[b] = (top_k_count / test_size)*100\n","  \n","#calculate SEs\n","prec1_se = standardError(prec1_bootstraps)\n","preck_se = standardError(preck_bootstraps)\n","print('\\n')\n","print(f'prec@1 se (%): {prec1_se}')\n","print(f'prec@6 se (%): {preck_se}')"],"metadata":{"id":"x46E6TBM_fQv","executionInfo":{"status":"ok","timestamp":1647105216014,"user_tz":-60,"elapsed":299967,"user":{"displayName":"Hiro French","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00499396809948838339"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1baf9a47-686f-4ca7-901c-46d2545ee648"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 200/200 [04:59<00:00,  1.50s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","\n","prec@1 se (%): 0.0274015016598457\n","prec@6 se (%): 0.05109871341913614\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["#bootstrap SEs for adjusted precision@ (takes appr. 30min)\n","\n","B = 200\n","np.random.seed(100)\n","prec1_bootstraps, preck_bootstraps = np.zeros(B),  np.zeros(B)\n","\n","for _,b in enumerate(tqdm(range(B),position=0, leave=True)):\n","\n","  idx_boot = np.random.choice(list(range(0,test_size)), size=test_size, replace=True)\n","  top_k_boot = [top_k_all[idx_boot[i],0:20] for i in range(test_size)]\n","  missing_items_boot = [missing_items_all[idx_boot[i]] for i in range(test_size)]\n","\n","  #context_names = getProductNames(incomplete_ids_test)\n","  top_k_names = getProductNames(top_k_boot)\n","  real_names = getProductNames_Real(missing_items_boot)\n","\n","  top_1_count,top_k_count = 0,0\n","  for i in range(test_size):\n","    real_name = real_names[i]\n","    top_k_item = top_k_names[i]\n","    if real_name == \"NOT AVAILABLE\":\n","      real_token = missing_items_boot[i]\n","      top_k_token = top_k_boot[i]\n","      if real_token == top_k_token[0]:\n","        top_1_count +=1\n","      for j in range(6):\n","        if real_token == top_k_token[j]:\n","          top_k_count += 1\n","          break\n","    else:\n","      predict_names, ngrams = [], {}\n","      q, t = 0,0\n","      ngram_correct = ngram(real_name)\n","      short=False\n","      while len(predict_names)!=6 and q!=len(top_k_item) and short!=True:\n","        if len(top_k_item)<=6:\n","          short=True\n","          predict_names = top_k_item\n","          for w, item in enumerate(predict_names):\n","            ngram_item = ngram(item)\n","            ngrams.update({w : ngram_item})\n","        else:\n","          item = top_k_item[q]   \n","          ngram_item = ngram(item)\n","          if q==0:\n","            predict_names.append(item)\n","            ngrams.update({t : ngram_item})\n","            t+=1\n","          else:\n","            sim = [dice_coef(ngram_item, value) for value in ngrams.values()]\n","            if all(z<params['ngram_sim'] for z in sim):\n","              ngrams.update({t : ngram_item})\n","              predict_names.append(item)\n","              t+=1\n","\n","          q+=1\n","        \n","      for key, value in ngrams.items():\n","        ngram_sim = dice_coef(ngram_correct, value)\n","        if ngram_sim >= params['ngram_sim']:\n","          output_want = [real_name] + predict_names\n","          output[total_batches-batch_size+i] = output_want\n","          top_k_count+=1\n","          if key==0:\n","            top_1_count+=1\n","          break\n","    prec1_bootstraps[b] = (top_1_count / test_size)*100\n","    preck_bootstraps[b] = (top_k_count / test_size)*100\n","  \n","#calculate SEs\n","prec1_se = standardError(prec1_bootstraps)\n","preck_se = standardError(preck_bootstraps)\n","print('\\n')\n","print(f'adj. prec@1 se (%): {prec1_se}')\n","print(f'adj. prec@6 se (%): {preck_se}')"],"metadata":{"id":"QutBzgEIZTNP","executionInfo":{"status":"ok","timestamp":1647034153850,"user_tz":-60,"elapsed":1397879,"user":{"displayName":"martijn korf","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08663037803752538794"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d391a2df-4dee-4c45-b379-7196e63e7adc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 200/200 [23:17<00:00,  6.99s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","\n","adj. prec@1 se (%): 0.1781470404031198\n","adj. prec@6 se (%): 0.2611987367586265\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["CODE FOR HISTOGRAMS (normal) prec@ vs. basket size"],"metadata":{"id":"dbMFdRKsldKX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YEBKBPHxR0zA","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["b1873999fab741cea69df27e530d5844","10d835b6f63542fc9641db612e490482","b8de0e85552d4b83928f9e3d6609fbb3","c5ee6c87dbb840bd8e29c24a5115a9b5","9470e65561964e978af005f87834291d","f489e03aa6a54a988114c4edc35ddb3a","3ee7fd97adfa4412a511b8d26deb2be4","f4676f1707d74e508204cde1dca2753e","9879df13204f4b2fb36f66aa327c79a2","dfb96a74ccc04016b1e85ce1c85c4ea8","65631578fa274659b1a9bf46d986b165"]},"executionInfo":{"status":"ok","timestamp":1647211912184,"user_tz":-60,"elapsed":62097,"user":{"displayName":"martijn korf","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08663037803752538794"}},"outputId":"0dc60fa1-1529-4786-b55b-aa0cf911a8d2"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1041 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1873999fab741cea69df27e530d5844"}},"metadata":{}}],"source":["#-------------------------------------------------------------------------------\n","# ~~~ TEST ~~~\n","#-------------------------------------------------------------------------------\n"," \n","loop_test = notebook.tqdm(loader_test, position=0, leave=True, colour='yellow')\n","correct_val = 0\n","num_specialtok = 5\n","item_num = -1\n","top_1_list,top_k_list,top_n_list,top_JOE_list = [],[],[],[]\n","for k, batch_test in enumerate(loop_test):\n","  batch_size = len(batch_test[0])\n","  non_predict = tf.fill((batch_size,num_specialtok),0.0)\n","\n","  incomplete_ids_test = batch_test[0]\n","  attention_mask_test = batch_test[1]\n","  labels = batch_test[2]\n","  missing_items_test = batch_test[3]\n","  incomplete_mask_test = batch_test[4]\n","\n","  outputBERT_logits = BERT.call(incomplete_ids_test, attention_mask=attention_mask_test, labels=labels,training=False).logits \n","  logits = []\n","  for m, mask_ind in enumerate(incomplete_mask_test.numpy()):\n","    lgts = outputBERT_logits[m,mask_ind,:]\n","    logits.append(lgts)\n","  outputBERT = tf.convert_to_tensor(logits)[:,num_specialtok:]\n","  outputBERT_softmax = tf.concat((non_predict, tf.nn.softmax(outputBERT)), axis=-1)\n","\n","  top_k = tf.nn.top_k(outputBERT_softmax, k=30, sorted=True)[1]\n","  top_k_items = tf.squeeze(top_k)\n","\n","  for i in range(0,batch_size):\n","    item_num += 1\n","    if (top_k_items[i][0] == missing_items_test[i]):\n","      top_1_list.append([batch_test['input_ids'][i],batch_test['attention_mask'][i],top_k_items[i],item_num])\n","      continue\n","    for j in range(1, params['n_seqs']):\n","      if (top_k_items[i][j] == missing_items_test[i]):\n","        top_k_list.append([batch_test['input_ids'][i],batch_test['attention_mask'][i],top_k_items[i],item_num])\n","        break\n","      if (j+1==params['n_seqs']):\n","        top_n_list.append([batch_test['input_ids'][i],batch_test['attention_mask'][i],top_k_items[i],item_num])\n","\n","  precK = correct_val/((k+1)*batch_size)\n","  loop_test.set_postfix(PrecisionAtK =precK, Correct=correct_val,  data=\"validation\")"]},{"cell_type":"code","source":["#print examples\n","\n","def print_predictions(batch,i):\n","\n","  context_names = getProductNames(batch[0][0:-1])\n","  real_names = getProductNames(batch[0][-1])\n","  top_k_names = getProductNames(batch[2])\n","  ID = batch[3]\n","  print(f'- {ID} - {i}')\n","  print(f'context: {context_names}')\n","  print(f'real name missing: {real_names}')\n","  print(f'top_k names: {top_k_names}')\n","  print('\\n')\n","\n","# print predictions, interchange the list with the one containing the desired predictions (top_1_list, top_k_list,top_n_list)\n","for i in range(0,500):\n"," print_predictions(top_1_list[i],i)"],"metadata":{"id":"VJNNnc01PS7z"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"BERT (AH) - Analysis on Test Sample.ipynb","provenance":[{"file_id":"1A3wTzYjqoZVuXfxNfdMTWjG3RAI8SvmW","timestamp":1643629509854},{"file_id":"15B9QJI6737fluhnsyoxkPnkW6Uoh459F","timestamp":1643117758680},{"file_id":"1oRfZnZhyObmIx0d_p7-AK0W5IM23lEsZ","timestamp":1642425862899},{"file_id":"1vZMOsayTPkcQ7Y97gJwwQKHqKr4_3BSn","timestamp":1642076413386}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b1873999fab741cea69df27e530d5844":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_10d835b6f63542fc9641db612e490482","IPY_MODEL_b8de0e85552d4b83928f9e3d6609fbb3","IPY_MODEL_c5ee6c87dbb840bd8e29c24a5115a9b5"],"layout":"IPY_MODEL_9470e65561964e978af005f87834291d"}},"10d835b6f63542fc9641db612e490482":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f489e03aa6a54a988114c4edc35ddb3a","placeholder":"​","style":"IPY_MODEL_3ee7fd97adfa4412a511b8d26deb2be4","value":" 48%"}},"b8de0e85552d4b83928f9e3d6609fbb3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4676f1707d74e508204cde1dca2753e","max":1041,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9879df13204f4b2fb36f66aa327c79a2","value":500}},"c5ee6c87dbb840bd8e29c24a5115a9b5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfb96a74ccc04016b1e85ce1c85c4ea8","placeholder":"​","style":"IPY_MODEL_65631578fa274659b1a9bf46d986b165","value":" 500/1041 [01:01&lt;01:02,  8.72it/s, Correct=0, PrecisionAtK=0, data=validation]"}},"9470e65561964e978af005f87834291d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f489e03aa6a54a988114c4edc35ddb3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ee7fd97adfa4412a511b8d26deb2be4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4676f1707d74e508204cde1dca2753e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9879df13204f4b2fb36f66aa327c79a2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":"yellow","description_width":""}},"dfb96a74ccc04016b1e85ce1c85c4ea8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65631578fa274659b1a9bf46d986b165":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}