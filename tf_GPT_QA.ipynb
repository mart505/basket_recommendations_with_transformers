{"cells":[{"cell_type":"markdown","metadata":{"id":"b7EStZqliOom"},"source":["**Empirical Analysis**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22437,"status":"ok","timestamp":1647273738615,"user":{"displayName":"Hiro French","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00499396809948838339"},"user_tz":-60},"id":"zUnWQLk_NQlv","outputId":"db3a16e4-86e3-479b-a8a3-1364cf0c185d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#Mount Drive for remote data access\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"eu6ce1MNM_g6"},"source":["# **Install packages**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ps4-gx1jMce9"},"outputs":[],"source":["!pip install transformers\n","!pip install tokenizers\n","!pip install tensorflow\n","!pip install tensorflow_addons\n","!pip install numba"]},{"cell_type":"markdown","metadata":{"id":"wZWH6TUGM6rB"},"source":["# **Import packages**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MhPiWNheNaw1"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow_addons.optimizers import AdamW\n","from tensorflow.python.client import device_lib\n","import matplotlib.pyplot as plt\n","import warnings\n","from tokenizers.processors import TemplateProcessing\n","from tokenizers.pre_tokenizers import WhitespaceSplit\n","from tokenizers.models import BPE\n","from tokenizers import Tokenizer\n","from tokenizers.trainers import BpeTrainer\n","from transformers import PreTrainedTokenizerFast, BertForPreTraining\n","from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n","from numba import cuda \n","from transformers import pipeline, set_seed, GPT2Model, GPT2Config, GPT2Tokenizer, TFGPT2LMHeadModel, TFBertForSequenceClassification, AdamW, BertConfig, TFBertForMaskedLM\n","from sklearn.model_selection import train_test_split, ShuffleSplit, KFold\n","from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n","import logging \n","logging.basicConfig(level=logging.ERROR)\n","import random\n","from math import *\n","from tqdm import tqdm, notebook\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os"]},{"cell_type":"markdown","metadata":{"id":"CtgDA4KBNelG"},"source":["# **Import and view data**"]},{"cell_type":"markdown","metadata":{"id":"fOXTK1cli59P"},"source":["Importing and Showing Data"]},{"cell_type":"code","source":["path_names = { 'train_data': \"/content/drive/MyDrive/Seminar QM/Data Cleaned/data_train_cln.txt\", \n","    'test_data': \"/content/drive/MyDrive/Seminar QM/Data/data_test.txt\",\n","    'tokenizer_file' : \"/content/drive/MyDrive/Seminar QM/Models/TF/Tokenizer/hamsbertje_wordlevel_cln.json\",\n","    'GPT_pretrained_model' : \"/content/drive/MyDrive/Seminar QM/Models/TF/220306 GPT pre-trained AH (REDDING)/GPT pre-trained\",\n","    'results_preck': \"results_precK_BERTbase\",\n","    'results_hyperparams': \"results_best-params_BERTbase\",\n","    'taxonomy': \"/content/drive/MyDrive/Seminar QM/Data/product_taxonomy_paths.csv\",\n","    'productID_toName': \"/content/drive/MyDrive/Seminar QM/Data/productIDs_names.txt\"\n","}\n","\n","params = {\n","    'max_tokenized_length' : 60,\n","    'batch_size' : 24,\n","    'n_seqs' : 6,\n","    'ngram_sim' : 0.8\n","}"],"metadata":{"id":"hzrhUDqLPSrU"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"executionInfo":{"elapsed":3139,"status":"ok","timestamp":1647273858529,"user":{"displayName":"Hiro French","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00499396809948838339"},"user_tz":-60},"id":"T5Rzu-K3NhFY","outputId":"b1cd4360-e788-4f6d-b1a0-7fee332714f4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                         prod_series\n","0                                167085 484999 59670\n","1  237907 163780 55068 222510 210737 139096 13909...\n","2             104818 140717 130862 41172 3856 124148"],"text/html":["\n","  <div id=\"df-87c154ad-5702-4557-9970-c15dd252a75c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>prod_series</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>167085 484999 59670</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>237907 163780 55068 222510 210737 139096 13909...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>104818 140717 130862 41172 3856 124148</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-87c154ad-5702-4557-9970-c15dd252a75c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-87c154ad-5702-4557-9970-c15dd252a75c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-87c154ad-5702-4557-9970-c15dd252a75c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}],"source":["#import from drive\n","df_org = pd.read_csv(path_names['train_data'], names=[\"prod_series\"], dtype=str, nrows=10000)\n","baskets = df_org['prod_series'].to_list()\n","\n","df_test = pd.read_csv(path_names['test_data'], names=[\"prod_series\"], dtype=str)\n","baskets_test = df_test['prod_series'].to_list()\n","\n","RANDOM_STATE = 123\n","random.seed(RANDOM_STATE)\n","tf.random.set_seed(RANDOM_STATE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S9sW4q1-ZaP6"},"outputs":[],"source":["basket_trial = []\n","for basket in df_test['prod_series']:\n","  step = []\n","  products = len(basket.split())\n","  for i in range(products):\n","    if i>0:\n","      basket_trial.append(basket.split()[0:i+1])\n","\n","baskets_trial = []\n","for i in basket_trial:\n","  step = \"\"\n","  for j in i:\n","    step = step +\" \" +  str(j)\n","  baskets_trial.append(step)\n","\n","df_trial = pd.DataFrame(baskets_trial)"]},{"cell_type":"markdown","metadata":{"id":"9gcbgoZYjGZb"},"source":["Importing Tokenizer and Creating BasketDataset Class"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2353,"status":"ok","timestamp":1647273969365,"user":{"displayName":"Hiro French","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00499396809948838339"},"user_tz":-60},"id":"1Vw4ypSu0dEM","outputId":"0a2788ea-c1ac-4c20-b368-d334a734e8ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["26722\n"]}],"source":["# loading tokenizer from the saved model path ; take old tokenizer\n","tokenizer = PreTrainedTokenizerFast(\n","    tokenizer_file=path_names['tokenizer_file'], # You can load from the tokenizer file, alternatively\n","    eos_token= \"</s>\",\n","    bos_token= \"<s>\",\n","    unk_token= \"<unk>\",\n","    pad_token= \"<pad>\",\n","    mask_token= \"<mask>\",\n","    padding_side = \"left\",\n","    truncation = \"left\",\n",")\n","\n","print(tokenizer.vocab_size)\n","\n","class BasketDataset:\n","    def __init__(self, baskets, tokenizer):\n","        self.tokenizer = tokenizer\n","        self.baskets = baskets\n","        self.input_ids = []\n","        self.attn_mask = []        \n","        self.encodings = self.tokenizer(self.baskets, truncation=True, max_length=params['max_tokenized_length'], add_special_tokens=False, padding=True)\n","        self.input_ids.append(tf.constant(self.encodings['input_ids']))\n","        self.attn_mask.append(tf.constant(self.encodings['attention_mask']))\n","    def __getitem__(self, idx):\n","        return self.input_ids[idx]\n","    def __len__(self):\n","        return len(self.input_ids)"]},{"cell_type":"markdown","metadata":{"id":"6VNENEyxYu-G"},"source":["# ***GPT BASE CASE***"]},{"cell_type":"markdown","metadata":{"id":"yy6qda2uqi7J"},"source":["Check and append GPU"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3048,"status":"ok","timestamp":1647273977630,"user":{"displayName":"Hiro French","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00499396809948838339"},"user_tz":-60},"id":"LPLUEnmD45wi","outputId":"9b45cee5-bbc5-4211-8fcc-fcfddfa2e164"},"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: /device:GPU:0\n"]}],"source":["# If there's a GPU available...\n","if tf.config.list_physical_devices('GPU'):    \n","    # Tell PyTorch to use the GPU.    \n","    device = tf.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % len(tf.config.list_physical_devices('GPU')))\n","    print('We will use the GPU:', device_lib.list_local_devices()[1].name)\n","    device_name = device_lib.list_local_devices()[1].name\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = tf.device(\"cpu\")\n","    device_name = device_lib.list_local_devices()[0].name"]},{"cell_type":"markdown","metadata":{"id":"Lekt3h4f97mp"},"source":["Initialize pre-trained models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lImprYsUvbxr"},"outputs":[],"source":["netG = TFGPT2LMHeadModel.from_pretrained(path_names['GPT_pretrained_model'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2VYLGkZVOanh"},"outputs":[],"source":["import json\n","with open(path_names['productID_toName']) as f:\n","    dict_textfile = f.read()     \n","productIDs_names = json.loads(dict_textfile)\n","\n","#returns nothing when product name not found\n","def getProductNames(A):\n","  if (tf.rank(A) <= 1): # check if rank 1, otherwise tokenizer.decode doesn't work\n","    B = tokenizer.decode(A).split(\" \")\n","    names = [productIDs_names[B[b]] for b in range(len(B)) if B[b] in productIDs_names.keys()]\n","  else:\n","    names = [getProductNames(A[i]) for i in range(len(A))]\n","  return names\n","\n","# returns \"n.a.\" when product name not found\n","def getProductNames_Real(A):\n","  if (tf.rank(A) <= 1): # check if rank 1, otherwise tokenizer.decode doesn't work\n","    B = tokenizer.decode(A).split(\" \")\n","    names = [productIDs_names[B[b]] if B[b] in productIDs_names.keys() else \"NOT AVAILABLE\" for b in range(len(B))]\n","  else:\n","    names = [getProductNames(A[i]) for i in range(len(A))]\n","  return names"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o5JwFNTpYrJj"},"outputs":[],"source":["def dice_coef(set1, set2):\n","  count = 0\n","  for i in set1:\n","    if i in set2:\n","      count+=1    \n","  dice =  2*count/(len(set1)+len(set2))\n","  return dice\n","\n","def ngram(product):\n","    n_gram = []\n","    for word in product.split():\n","        for j in range(len(word)-2): \n","          if len(word)>=3:   \n","              n_gram.append(word[j: j + 3])\n","    return n_gram\n","\n","def StandardError(bootstraps):\n","  B = len(bootstraps)\n","  return np.sqrt(1/(B-1) * sum( np.square(bootstraps - np.mean(bootstraps))))"]},{"cell_type":"markdown","metadata":{"id":"nBBpXBGUpYQK"},"source":["TEST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7pFQLQQwXicR"},"outputs":[],"source":["global dataset_test\n","baskets_dataset = baskets_test # insert baskets_trails for sequential results\n","dataset_test = BasketDataset(baskets_dataset, tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":131,"referenced_widgets":["400a41036c15489db47433880f7f1698","86c5573fee644428b0ee123eac6115db","dc7d0b41b9484597addab60a3ea00841","489e4f10ac564c069e541ff3bf959f4c","479222b1ad694bb5bd46ca3e293b262d","5a7e644f4ec84e44bf3ab1d20fcc61b0","e9d6e664920245ada8703edf298da8e4","31bbff3429f644d3a54cb645e2836956","e4d8686777b449eb892f16f0e2b4e108","eaaf4cd632c64b598314e8a656e28305","56140463804d45bdb0252d99d41142e3"]},"executionInfo":{"elapsed":443343,"status":"ok","timestamp":1647274498326,"user":{"displayName":"Hiro French","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00499396809948838339"},"user_tz":-60},"id":"spbU1pLq5Rde","outputId":"289de6bf-fac3-41b1-cea6-676b93f3ecf7"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1042 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"400a41036c15489db47433880f7f1698"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["   Mean Percentile Rank: 0.9146133093284213\n","     precision at 1 (%): 4.708188327533101\n","     precision at 6 (%): 12.81251250050002\n","adj. precision at 1 (%): 10.78043121724869\n","adj. precision at 6 (%): 23.084923396935878\n"]}],"source":["#-------------------------------------------------------------------------------\n","# ~~~ TEST ~~~ w/ bootstrap for stats\n","#-------------------------------------------------------------------------------\n","\n","test_size = len(baskets_dataset)\n","num_specialtok = 5\n","top_1_count,top_k_count = 0,0\n","top_1_count_adj, top_k_count_adj = 0,0\n","output = {}\n","total_batches = 0\n","pr_ls_test = []\n","\n","test_basket = {}\n","test_basket['input_ids'] = dataset_test.input_ids[0]\n","test_basket['attention_mask'] = dataset_test.attn_mask[0]\n","loader_test = tf.data.Dataset.from_tensor_slices(test_basket).batch(params['batch_size'])   \n","loop_test = notebook.tqdm(loader_test, position=0, leave=True, colour='yellow')\n","\n","#create top_k matrix for predictions in test set\n","top_k_all = np.zeros((test_size,30),dtype=np.int32)\n","missing_items_all = np.zeros((test_size),dtype=np.int32)\n","top_1_list,top_k_list,top_n_list = [],[],[]\n","\n","for k, batch_test in enumerate(loop_test):\n","  batch_size = len(batch_test['input_ids'])\n","  total_batches += batch_size\n","  non_predict = tf.fill((batch_size,num_specialtok),0.0)\n","  incomplete_ids_test = batch_test['input_ids'][:,0:-1]\n","  incomplete_mask_test = batch_test['attention_mask'][:,0:-1]\n","  missing_items_test = batch_test['input_ids'][:,-1]\n","  outputG = netG.call(input_ids=incomplete_ids_test,\n","                      attention_mask=incomplete_mask_test,\n","                      training = False).logits[:,-1,num_specialtok:]\n","\n","  #model should not predict special tokens, therefore, set softmax value at zero\n","  outputG_softmax = tf.concat((non_predict, tf.nn.softmax(outputG)), axis=-1)\n","  top_k = tf.nn.top_k(outputG_softmax, k=30, sorted=True)[1]\n","  top_k_items = tf.squeeze(top_k)\n","  for i in range(batch_size):\n","    top_k_all[(k*batch_size)+i,:] =  top_k_items[i]\n","    missing_items_all[(k*batch_size)+i] = missing_items_test[i]\n","\n","  # calculate (normal) prec@k\n","  for i in range(0,batch_size):\n","    top_n_list.append([batch_test['input_ids'][i],batch_test['attention_mask'][i],top_k_items[i]])\n","    top_1_count += int(top_k_items[i][0] == missing_items_test[i])\n","    for j in range(6):\n","      if (top_k_items[i][j] == missing_items_test[i]):\n","        top_k_count += 1\n","        break\n","\n","  # calculate adj. prec@k\n","  top_k_names = getProductNames(top_k_items)\n","  real_names = getProductNames_Real(missing_items_test)\n","  for i in range(batch_size):\n","    real_name = real_names[i]\n","    top_k_item = top_k_names[i]\n","    if real_name == \"NOT AVAILABLE\":\n","      real_token = missing_items_test[i]\n","      top_k_token = top_k_items[i]\n","      if real_token == top_k_token[0]:\n","        top_1_count_adj +=1\n","      for j in range(6):\n","        if real_token == top_k_token[j]:\n","          top_k_count_adj += 1\n","          break\n","    else:\n","      predict_names, ngrams = [], dict()\n","      q, t = 0,0\n","      ngram_correct = ngram(real_name)\n","      short=False\n","      while len(predict_names)!=6 and q!=len(top_k_item) and short!=True:\n","        if len(top_k_item)<=6:\n","          short=True\n","          predict_names = top_k_item\n","          for w, item in enumerate(predict_names):\n","            ngram_item = ngram(item)\n","            ngrams.update({w : ngram_item})\n","        else:\n","          item = top_k_item[q]   \n","          ngram_item = ngram(item)\n","          if q==0:\n","            predict_names.append(item)\n","            ngrams.update({t : ngram_item})\n","            t+=1\n","          else:\n","            sim = [dice_coef(ngram_item, value) for value in ngrams.values()]\n","            if all(z<params['ngram_sim'] for z in sim):\n","              ngrams.update({t : ngram_item})\n","              predict_names.append(item)\n","              t+=1\n","\n","          q+=1\n","        \n","      for key, value in ngrams.items():\n","        ngram_sim = dice_coef(ngram_correct, value)\n","        if ngram_sim >= params['ngram_sim']:\n","          output_want = [real_name] + predict_names\n","          output[total_batches-batch_size+i] = output_want\n","          top_k_count_adj+=1\n","          top_k_list.append([batch_test['input_ids'][i],batch_test['attention_mask'][i],top_k_items[i]])\n","          if key==0:\n","            top_1_count_adj+=1\n","            top_1_list.append([batch_test['input_ids'][i],batch_test['attention_mask'][i],top_k_items[i]])\n","          break\n","  \n","  for i in range(0,batch_size):\n","    pr = tf.math.reduce_sum(tf.where(tf.math.less(outputG_softmax[i,:], outputG_softmax[i,missing_items_test[i].numpy()]), 1.0, 0.0)).numpy()/tokenizer.vocab_size\n","    pr_ls_test.append(pr)\n","  precK_test = top_k_count/(total_batches)\n","\n","  mpr_test = np.array(pr_ls_test).sum()/(total_batches) \n","\n","  loop_test.set_postfix(PrecisionAtK =round(precK_test,4), Correct=top_k_count,  data=\"Test\", MPR = mpr_test)\n","\n","#print metrics\n","mpr_test = np.array(pr_ls_test).sum()/test_size\n","prec1 = (top_1_count/test_size) * 100\n","prec6 = (top_k_count/test_size) * 100\n","prec1_adj = (top_1_count_adj/test_size) * 100\n","prec6_adj = ((top_k_count_adj)/test_size) * 100\n","print(f\"   Mean Percentile Rank: {mpr_test}\")\n","print(f\"     precision at 1 (%): {prec1}\")\n","print(f\"     precision at 6 (%): {prec6}\")\n","print(f\"adj. precision at 1 (%): {prec1_adj}\")\n","print(f\"adj. precision at 6 (%): {prec6_adj}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15521,"status":"ok","timestamp":1647263316446,"user":{"displayName":"martijn korf","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08663037803752538794"},"user_tz":-60},"id":"x46E6TBM_fQv","outputId":"4ec3b1fb-1e77-4293-e7fa-4e2516b1908b"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 200/200 [00:15<00:00, 12.74it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","prec@1 se (%): 0.11697136483030471\n","prec@6 se (%): 0.2176926111264603\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["#bootstrap SEs for (normal) precision@ (takes appr. 30 seconds)\n","\n","B = 200\n","np.random.seed(100)\n","prec1_bootstraps, preck_bootstraps = np.zeros(B),  np.zeros(B)\n","\n","for _,b in enumerate(tqdm(range(B),position=0, leave=True)):\n","  #create bootstrap\n","  idx_boot = np.random.choice(list(range(0,test_size)), size=test_size, replace=True)\n","  top_k_boot = [top_k_all[idx_boot[i],0:6] for i in range(test_size)]\n","  missing_items_boot = [missing_items_all[idx_boot[i]] for i in range(test_size)]\n","\n","  # calculate bootstrap stats\n","  top_1_count,top_k_count = 0,0\n","  for i in range(0,test_size):\n","    top_1_count += int(top_k_boot[i][0] == missing_items_boot[i])\n","    for j in range(6):\n","      if (top_k_boot[i][j] == missing_items_boot[i]):\n","        top_k_count += 1\n","        break\n","  prec1_bootstraps[b] = (top_1_count / test_size)*100\n","  preck_bootstraps[b] = (top_k_count / test_size)*100\n","  \n","#calculate SEs\n","prec1_se = StandardError(prec1_bootstraps)\n","preck_se = StandardError(preck_bootstraps)\n","print('\\n')\n","print(f'prec@1 se (%): {prec1_se}')\n","print(f'prec@6 se (%): {preck_se}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"QutBzgEIZTNP","outputId":"ca56b128-8068-4ac0-9f94-83b1c7a11a5f"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 200/200 [22:37<00:00,  6.79s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","adj. prec@1 se (%): 0.19158802570849098\n","adj. prec@6 se (%): 0.27646718458756797\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["#bootstrap SEs for adjusted precision@ (takes appr. 30min)\n","\n","B = 200\n","np.random.seed(100)\n","prec1_bootstraps, preck_bootstraps = np.zeros(B),  np.zeros(B)\n","\n","for _,b in enumerate(tqdm(range(B),position=0, leave=True)):\n","\n","  idx_boot = np.random.choice(list(range(0,test_size)), size=test_size, replace=True)\n","  top_k_boot = [top_k_all[idx_boot[i],0:20] for i in range(test_size)]\n","  missing_items_boot = [missing_items_all[idx_boot[i]] for i in range(test_size)]\n","\n","  #context_names = getProductNames(incomplete_ids_test)\n","  top_k_names = getProductNames(top_k_boot)\n","  real_names = getProductNames_Real(missing_items_boot)\n","\n","  top_1_count,top_k_count = 0,0\n","  for i in range(test_size):\n","    real_name = real_names[i]\n","    top_k_item = top_k_names[i]\n","    if real_name == \"NOT AVAILABLE\":\n","      real_token = missing_items_boot[i]\n","      top_k_token = top_k_boot[i]\n","      if real_token == top_k_token[0]:\n","        top_1_count +=1\n","      for j in range(6):\n","        if real_token == top_k_token[j]:\n","          top_k_count += 1\n","          break\n","    else:\n","      predict_names, ngrams = [], {}\n","      q, t = 0,0\n","      ngram_correct = ngram(real_name)\n","      short=False\n","      while len(predict_names)!=6 and q!=len(top_k_item) and short!=True:\n","        if len(top_k_item)<=6:\n","          short=True\n","          predict_names = top_k_item\n","          for w, item in enumerate(predict_names):\n","            ngram_item = ngram(item)\n","            ngrams.update({w : ngram_item})\n","        else:\n","          item = top_k_item[q]   \n","          ngram_item = ngram(item)\n","          if q==0:\n","            predict_names.append(item)\n","            ngrams.update({t : ngram_item})\n","            t+=1\n","          else:\n","            sim = [dice_coef(ngram_item, value) for value in ngrams.values()]\n","            if all(z<params['ngram_sim'] for z in sim):\n","              ngrams.update({t : ngram_item})\n","              predict_names.append(item)\n","              t+=1\n","\n","          q+=1\n","          #if q>=params['n_seqs']-1:\n","          #  print(\"ALERT\")\n","        \n","      for key, value in ngrams.items():\n","        ngram_sim = dice_coef(ngram_correct, value)\n","        if ngram_sim >= params['ngram_sim']:\n","          output_want = [real_name] + predict_names\n","          output[total_batches-batch_size+i] = output_want\n","          top_k_count+=1\n","          if key==0:\n","            top_1_count+=1\n","          break\n","    prec1_bootstraps[b] = (top_1_count / test_size)*100\n","    preck_bootstraps[b] = (top_k_count / test_size)*100\n","  \n","#calculate SEs\n","prec1_se = StandardError(prec1_bootstraps)\n","preck_se = StandardError(preck_bootstraps)\n","print('\\n')\n","print(f'adj. prec@1 se (%): {prec1_se}')\n","print(f'adj. prec@6 se (%): {preck_se}')"]},{"cell_type":"markdown","metadata":{"id":"dbMFdRKsldKX"},"source":["CODE FOR HISTOGRAMS (normal) prec@ vs. basket size"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["ef6cd26d02cb490cae682d6b6f519b6c","767c2dbb93f344378cdc11a6163b31ad","77a97f039d2546dba44607b664eed0fb","d3776bdca5a749fd826b277cfac65e3a","4cc7a097dba24713a3f7a932ac0c0f1c","c217d201873d427cb298399ed2a58dd0","3ffeca4eb1e446f0af22303280ed5347","5815a1fe393943bc8c365c418d4df2ab","3eb8b3f9bbf24ad69c50327ad02fc56a","b28b83d6730c46d683d38cb0f7e13995","d7cf045850a94f39b03f244b4a0f1bc1"]},"executionInfo":{"elapsed":326414,"status":"ok","timestamp":1647274884372,"user":{"displayName":"Hiro French","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00499396809948838339"},"user_tz":-60},"id":"YEBKBPHxR0zA","outputId":"217e19c6-b676-4760-b5cf-28ec14547e92"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1042 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef6cd26d02cb490cae682d6b6f519b6c"}},"metadata":{}}],"source":["#-------------------------------------------------------------------------------\n","# ~~~ TEST ~~~\n","#-------------------------------------------------------------------------------\n","\n","global dataset_test\n","dataset_test = BasketDataset(baskets_test, tokenizer)\n","\n","test_basket = {}\n","test_basket['input_ids'] = dataset_test.input_ids[0]\n","test_basket['attention_mask'] = dataset_test.attn_mask[0]\n","\n","top_1_list,top_k_list,top_n_list = [],[],[]\n","\n","loader_test = tf.data.Dataset.from_tensor_slices(test_basket).batch(params['batch_size'])   \n","loop_test = notebook.tqdm(loader_test, position=0, leave=True, colour='yellow')\n","correct_val, item_num = 0,-1\n","\n","for k, batch_test in enumerate(loop_test):\n","  batch_size = len(batch_test['input_ids'])\n","  incomplete_ids_test = batch_test['input_ids'][:,0:-1]\n","  incomplete_mask_test = batch_test['attention_mask'][:,0:-1]\n","  missing_items_test = batch_test['input_ids'][:,-1]\n","  outputG = netG.call(input_ids=incomplete_ids_test,\n","                      attention_mask=incomplete_mask_test)\n","  outputG_softmax = tf.nn.softmax(outputG.logits[:,-1,:])\n","  top_k = tf.nn.top_k(outputG_softmax, k=params['n_seqs'], sorted=True)[1]\n","  top_k_items = tf.squeeze(top_k)\n","\n","  for i in range(0,batch_size):\n","    item_num += 1\n","    if (top_k_items[i][0] == missing_items_test[i]):\n","      top_1_list.append([batch_test['input_ids'][i],batch_test['attention_mask'][i],top_k_items[i],item_num])\n","      continue\n","    for j in range(1, params['n_seqs']):\n","      if (top_k_items[i][j] == missing_items_test[i]):\n","        top_k_list.append([batch_test['input_ids'][i],batch_test['attention_mask'][i],top_k_items[i],item_num])\n","        break\n","      if (j+1==params['n_seqs']):\n","        top_n_list.append([batch_test['input_ids'][i],batch_test['attention_mask'][i],top_k_items[i],item_num])\n","\n","  precK = correct_val/((k+1)*batch_size)\n","  loop_test.set_postfix(PrecisionAtK =precK, Correct=correct_val,  data=\"validation\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XP5-m1jDSBN4"},"outputs":[],"source":["# Investigate basket length vs out-of-sample performance\n","\n","#create dictionaries with basket length and frequency for prec@1, prec@6 and whole test set\n","list_len_1 = [ sum(top_1_list[i][1]).numpy() for i in range(len(top_1_list))]\n","dict_len_1 = { i: list_len_1.count(i) for i in range(1,101)}\n","list_len_k = [ sum(top_k_list[i][1]) for i in range(len(top_k_list))]\n","dict_len_k = { i: list_len_k.count(i) for i in range(1,101)}\n","list_len_n = [ sum(top_n_list[i][1]) for i in range(len(top_n_list))]\n","dict_len_n = { i: list_len_n.count(i) for i in range(1,101)}\n","\n","# normalise the results as for total occurence in dataset (there are a lot more small baskets in the data)\n","# use binwidth h = 2 as to smooth results\n","dict_total = { i: dict_len_k[i] + dict_len_n[i] for i in range(1,61)}\n","print(dict_total)\n","dict_top1_relative = { i: ((dict_len_1[i] + dict_len_1[i+1]) / (dict_total[i+1] + dict_total[i])) if ((i+1 % 2) == 0) else ((dict_len_1[i] + dict_len_1[i-1]) / (dict_total[i-1] + dict_total[i])) for i in range(2,61)}\n","print(dict_top1_relative)\n","dict_topk_relative = { i: ((dict_len_k[i] + dict_len_k[i+1]) / (dict_total[i+1] + dict_total[i])) if ((i+1 % 2) == 0) else ((dict_len_k[i] + dict_len_k[i-1]) / (dict_total[i-1] + dict_total[i])) for i in range(2,61)}\n","\n","# plot results\n","plt.figure(0)\n","a = plt.bar(dict_top1_relative.keys(), dict_top1_relative.values(), width=0.5, color='tab:blue', label='prec @ 1')\n","plt.figure(1)\n","plt.bar(dict_topk_relative.keys(), dict_topk_relative.values(), width=0.5, color='tab:orange', label='prec @ k')\n","plt.figure(2)\n","plt.bar(dict_total.keys(), dict_total.values(), width=0.5, color='tab:blue')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VJNNnc01PS7z"},"outputs":[],"source":["#print examples\n","\n","def print_predictions(batch,i):\n","\n","  context_names = getProductNames(batch[0][0:-1])\n","  real_names = getProductNames(batch[0][-1])\n","  top_k_names = getProductNames(batch[2])\n","  ID = batch[3]\n","  print(f'- {ID} - {i}')\n","  print(f'context: {context_names}')\n","  print(f'real name missing: {real_names}')\n","  print(f'top_k names: {top_k_names}')\n","  print('\\n')\n","\n","\n","# print predictions, interchange the list with the one containing the desired predictions (top_1_list, top_k_list,top_n_list)\n","for i in range(0,500):\n"," print_predictions(top_1_list[i],i)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"GPT (AH) - Analysis on Test Sample.ipynb","provenance":[{"file_id":"1A3wTzYjqoZVuXfxNfdMTWjG3RAI8SvmW","timestamp":1643629509854},{"file_id":"15B9QJI6737fluhnsyoxkPnkW6Uoh459F","timestamp":1643117758680},{"file_id":"1oRfZnZhyObmIx0d_p7-AK0W5IM23lEsZ","timestamp":1642425862899},{"file_id":"1vZMOsayTPkcQ7Y97gJwwQKHqKr4_3BSn","timestamp":1642076413386}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"400a41036c15489db47433880f7f1698":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_86c5573fee644428b0ee123eac6115db","IPY_MODEL_dc7d0b41b9484597addab60a3ea00841","IPY_MODEL_489e4f10ac564c069e541ff3bf959f4c"],"layout":"IPY_MODEL_479222b1ad694bb5bd46ca3e293b262d"}},"86c5573fee644428b0ee123eac6115db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a7e644f4ec84e44bf3ab1d20fcc61b0","placeholder":"​","style":"IPY_MODEL_e9d6e664920245ada8703edf298da8e4","value":"100%"}},"dc7d0b41b9484597addab60a3ea00841":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_31bbff3429f644d3a54cb645e2836956","max":1042,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e4d8686777b449eb892f16f0e2b4e108","value":1042}},"489e4f10ac564c069e541ff3bf959f4c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eaaf4cd632c64b598314e8a656e28305","placeholder":"​","style":"IPY_MODEL_56140463804d45bdb0252d99d41142e3","value":" 1042/1042 [07:22&lt;00:00,  2.83it/s, Correct=3203, MPR=0.915, PrecisionAtK=0.128, data=Test]"}},"479222b1ad694bb5bd46ca3e293b262d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a7e644f4ec84e44bf3ab1d20fcc61b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9d6e664920245ada8703edf298da8e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"31bbff3429f644d3a54cb645e2836956":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4d8686777b449eb892f16f0e2b4e108":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":"yellow","description_width":""}},"eaaf4cd632c64b598314e8a656e28305":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56140463804d45bdb0252d99d41142e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ef6cd26d02cb490cae682d6b6f519b6c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_767c2dbb93f344378cdc11a6163b31ad","IPY_MODEL_77a97f039d2546dba44607b664eed0fb","IPY_MODEL_d3776bdca5a749fd826b277cfac65e3a"],"layout":"IPY_MODEL_4cc7a097dba24713a3f7a932ac0c0f1c"}},"767c2dbb93f344378cdc11a6163b31ad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c217d201873d427cb298399ed2a58dd0","placeholder":"​","style":"IPY_MODEL_3ffeca4eb1e446f0af22303280ed5347","value":"100%"}},"77a97f039d2546dba44607b664eed0fb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5815a1fe393943bc8c365c418d4df2ab","max":1042,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3eb8b3f9bbf24ad69c50327ad02fc56a","value":1042}},"d3776bdca5a749fd826b277cfac65e3a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b28b83d6730c46d683d38cb0f7e13995","placeholder":"​","style":"IPY_MODEL_d7cf045850a94f39b03f244b4a0f1bc1","value":" 1042/1042 [05:22&lt;00:00,  3.82it/s, Correct=0, PrecisionAtK=0, data=validation]"}},"4cc7a097dba24713a3f7a932ac0c0f1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c217d201873d427cb298399ed2a58dd0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ffeca4eb1e446f0af22303280ed5347":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5815a1fe393943bc8c365c418d4df2ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3eb8b3f9bbf24ad69c50327ad02fc56a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":"yellow","description_width":""}},"b28b83d6730c46d683d38cb0f7e13995":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7cf045850a94f39b03f244b4a0f1bc1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}