{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21934,"status":"ok","timestamp":1647429790332,"user":{"displayName":"Ron Hochstenbach","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giej6vic0ke4BMLFKMMDWFqUyl-yaVHYSAhYOFk2w=s64","userId":"09974895698778930329"},"user_tz":-60},"id":"zUnWQLk_NQlv","outputId":"bf3ab592-b8db-4003-f815-9cd0d2292c66"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#Mount Drive for remote data access\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"eu6ce1MNM_g6"},"source":["# **Install packages**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ps4-gx1jMce9"},"outputs":[],"source":["!pip install transformers\n","!pip install tokenizers\n","!pip install tensorflow\n","!pip install tensorflow_addons"]},{"cell_type":"markdown","metadata":{"id":"wZWH6TUGM6rB"},"source":["# **Import packages**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MhPiWNheNaw1"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow_addons.optimizers import AdamW\n","from tensorflow.python.client import device_lib\n","\n","import matplotlib.pyplot as plt\n","import warnings\n","from tokenizers.processors import TemplateProcessing\n","from tokenizers.pre_tokenizers import WhitespaceSplit\n","from tokenizers.models import BPE\n","from tokenizers import Tokenizer\n","from tokenizers.trainers import BpeTrainer\n","from transformers import PreTrainedTokenizerFast, BertForPreTraining\n","from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n","from numba import cuda \n","from transformers import pipeline, set_seed, GPT2Model, GPT2Config, GPT2Tokenizer, TFGPT2LMHeadModel, TFBertForSequenceClassification, AdamW, BertConfig, TFBertForMaskedLM\n","\n","from sklearn.model_selection import train_test_split, ShuffleSplit, KFold\n","from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n","import logging \n","logging.basicConfig(level=logging.ERROR)\n","import random\n","from math import *\n","from tqdm import tqdm, notebook\n","import pandas as pd\n","import numpy as np\n","\n","import os\n","\n","RANDOM_STATE = 123\n","random.seed(RANDOM_STATE)\n","tf.random.set_seed(RANDOM_STATE)"]},{"cell_type":"markdown","metadata":{"id":"yy6qda2uqi7J"},"source":["Check and append GPU"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1647097272094,"user":{"displayName":"Maarten de Ruiter","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02394617415385259921"},"user_tz":-60},"id":"LPLUEnmD45wi","outputId":"2e8c8bc5-4685-4c6f-9625-e209627a1bec"},"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: /device:GPU:0\n"]}],"source":["# If there's a GPU available...\n","if tf.config.list_physical_devices('GPU'):    \n","    # Tell PyTorch to use the GPU.    \n","    device = tf.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % len(tf.config.list_physical_devices('GPU')))\n","    print('We will use the GPU:', device_lib.list_local_devices()[1].name)\n","    device_name = device_lib.list_local_devices()[1].name\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = tf.device(\"cpu\")\n","    device_name = device_lib.list_local_devices()[0].name"]},{"cell_type":"markdown","metadata":{"id":"CtgDA4KBNelG"},"source":["# **Import and view data**"]},{"cell_type":"code","source":["#Pathnames\n","\n","path_names = { 'train_data': \"/content/drive/MyDrive/Seminar QM/Data Cleaned/data_train_cln.txt\", \n","    'test_data': \"/content/drive/MyDrive/Seminar QM/Data/data_test.txt\",\n","    'tokenizer_file' : \"/content/drive/MyDrive/Seminar QM/Models/TF/Tokenizer/hamsbertje_wordlevel_cln.json\",\n","    'BERT_pretrained_model' : \"/content/drive/MyDrive/Seminar QM/Models/TF/220306 BERT pre-trained AH (REDDING)/bert pre-trained\",\n","    'results_preck': \"results_precK_BERTbase\",\n","    'results_hyperparams': \"results_best-params_BERTbase\"\n","}"],"metadata":{"id":"WKzk9erKex7a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fOXTK1cli59P"},"source":["Importing and Showing Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T5Rzu-K3NhFY"},"outputs":[],"source":["#import from drive\n","n_rows_train = 1000   #This controls the number of train baskets to load in\n","df_org = pd.read_csv(path_names['train_data'], names=[\"prod_series\"], dtype=str, nrows=n_rows_train)\n","baskets = df_org['prod_series'].to_list()\n","\n","n_rows_test = 1000   #This controls the number of test baskets to load in\n","df_test = pd.read_csv(path_names['test_data'], names=[\"prod_series\"], dtype=str,nrows=n_rows_test)\n","baskets_test = df_test['prod_series'].to_list()"]},{"cell_type":"markdown","metadata":{"id":"9gcbgoZYjGZb"},"source":["Importing Tokenizer and Tokenization\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Vw4ypSu0dEM"},"outputs":[],"source":["# loading tokenizer from the saved model path\n","tokenizer = PreTrainedTokenizerFast(\n","    tokenizer_file=path_names['tokenizer_file'], # You can load from the tokenizer file, alternatively\n","    eos_token= \"</s>\",\n","    bos_token= \"<s>\",\n","    unk_token= \"<unk>\",\n","    pad_token= \"<pad>\",\n","    mask_token= \"<mask>\",\n","    truncation_side = 'left'\n",")"]},{"cell_type":"markdown","metadata":{"id":"KyJy2jiQoZoP"},"source":["# **BERT BASE CASE**"]},{"cell_type":"markdown","metadata":{"id":"Uf5t4i_LspRk"},"source":["# BERT Hyperparamaters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6TQ3XH0s3YB"},"outputs":[],"source":["#Hyperparams\n","bayes_parameters = {'lr' : hp.uniform('lr', 1e-7, 6e-5),\n","                    'epochs' : hp.quniform('epochs', 3,5,1)}\n","\n","params = {\n","    'max_tokenized_length' : 60,\n","    'batch_size' : 12,\n","    'n_seqs' : 6,\n","    'n_folds' : 3\n","}"]},{"cell_type":"markdown","metadata":{"id":"201q2qHLCE6j"},"source":["Prepare data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5dxG7fwMG2MW"},"outputs":[],"source":["dataset = tokenizer(baskets, return_tensors='tf',\n","          max_length=params['max_tokenized_length'], truncation=True, padding='max_length')\n","\n","dataset_test = tokenizer(baskets_test, return_tensors='tf',\n","               max_length=params['max_tokenized_length'], truncation=True, padding='max_length')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nudT0I-IQAxP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647430829528,"user_tz":-60,"elapsed":3602,"user":{"displayName":"Ron Hochstenbach","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giej6vic0ke4BMLFKMMDWFqUyl-yaVHYSAhYOFk2w=s64","userId":"09974895698778930329"}},"outputId":"731819b7-f8d4-44cd-c26b-4e9037573917"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [00:03<00:00, 291.82it/s]\n"]}],"source":["#Append labels, masking, missing items\n","#Train\n","indices_to_mask = np.where(dataset.input_ids == 2)[1]-1\n","\n","labels_tensors = []\n","missing_items = []\n","mask_tensors = []\n","\n","for i in tqdm(range(tf.shape(dataset.input_ids)[0].numpy())):\n","  #labels\n","  labels_array = np.full(tf.shape(dataset.input_ids)[1].numpy(), -100, dtype=np.int32)\n","  labels_array[indices_to_mask[i]] = dataset.input_ids[i,indices_to_mask[i]]\n","  labels_tensors.append(labels_array)\n","  #missing items\n","  missing_items.append([dataset.input_ids[i,indices_to_mask[i]]])\n","  #masking\n","  bool_array = np.zeros(tf.shape(dataset.input_ids)[1].numpy(),dtype=bool)\n","  bool_array[indices_to_mask[i]] = True\n","  mask_tensors.append(bool_array)\n","\n","labels_arr = tf.convert_to_tensor(labels_tensors)\n","missing_item_arr = tf.convert_to_tensor(missing_items)\n","mask_arr = tf.convert_to_tensor(mask_tensors)\n","\n","dataset['labels'] = labels_arr\n","dataset['missing_items'] = missing_item_arr\n","dataset['input_ids'] = tf.where(~mask_arr, dataset.input_ids, 4)\n","dataset['masked_index'] = tf.convert_to_tensor(np.where(dataset['labels'] != -100)[1])"]},{"cell_type":"code","source":["#Append labels, masking, missing items\n","#Test\n","indices_to_mask_test = np.where(dataset_test.input_ids == 2)[1]-1\n","\n","labels_tensors_test = []\n","missing_items_test = []\n","mask_tensors_test = []\n","for i in tqdm(range(tf.shape(dataset_test.input_ids)[0].numpy())):\n","  #labels\n","  labels_array_test = np.full(tf.shape(dataset_test.input_ids)[1].numpy(), -100, dtype=np.int32)\n","  labels_array_test[indices_to_mask_test[i]] = dataset_test.input_ids[i,indices_to_mask_test[i]]\n","  labels_tensors_test.append(labels_array_test)\n","  #missing items\n","  missing_items_test.append([dataset_test.input_ids[i,indices_to_mask_test[i]]])\n","  #masking\n","  bool_array_test = np.zeros(tf.shape(dataset_test.input_ids)[1].numpy(),dtype=bool)\n","  bool_array_test[indices_to_mask_test[i]] = True\n","  mask_tensors_test.append(bool_array_test)\n","\n","labels_arr_test = tf.convert_to_tensor(labels_tensors_test)\n","missing_item_arr_test = tf.convert_to_tensor(missing_items_test)\n","mask_arr_test = tf.convert_to_tensor(mask_tensors_test)\n","\n","dataset_test['labels'] = labels_arr_test\n","dataset_test['missing_items'] = missing_item_arr_test\n","dataset_test['input_ids'] = tf.where(~mask_arr_test, dataset_test.input_ids, 4)\n","dataset_test['masked_index'] = tf.convert_to_tensor(np.where(dataset_test['labels'] != -100)[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pnioQV0MIYCG","executionInfo":{"status":"ok","timestamp":1647430846549,"user_tz":-60,"elapsed":1880,"user":{"displayName":"Ron Hochstenbach","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giej6vic0ke4BMLFKMMDWFqUyl-yaVHYSAhYOFk2w=s64","userId":"09974895698778930329"}},"outputId":"946cf23e-1ab3-467c-95d4-f0ecbd0eb42c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [00:01<00:00, 572.59it/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"Lekt3h4f97mp"},"source":["Initialize pre-trained models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lImprYsUvbxr"},"outputs":[],"source":["global criterion\n","global bad_words\n","\n","criterion = tf.keras.losses.CategoricalCrossentropy(from_logits=False, axis=-1)\n","bad_words = [[tokenizer.eos_token_id],[tokenizer.bos_token_id], [tokenizer.pad_token_id]]\n","\n","kfold = KFold(n_splits=params['n_folds'], shuffle=True, random_state=RANDOM_STATE)"]},{"cell_type":"markdown","metadata":{"id":"zo-JDIdxyQ4W"},"source":["## TF BERT as Base Case"]},{"cell_type":"markdown","source":["Hyperparameter optimization"],"metadata":{"id":"lRW8atPpw2Et"}},{"cell_type":"code","source":["def train_step(batch, optim,BERT):\n","\n","    input_ids = batch[0]\n","    attention_mask = batch[1]\n","    labels = batch[2]\n","    missing_items = batch[3]\n","    masked_index = batch[4]\n","\n","    with tf.GradientTape() as tape:\n","      res = BERT.call(input_ids, attention_mask=attention_mask, labels=labels, training=True)\n","      loss=tf.math.reduce_mean(res.loss)\n","      attentions=res.attentions\n","    optim.minimize(loss, var_list = BERT.trainable_variables, tape=tape)\n","\n","    return loss,attentions"],"metadata":{"id":"DZVd-tC1CHce"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jFfqjdD0yPev"},"outputs":[],"source":["def BERT_Base_finetuning(space):\n","  lr = space['lr']\n","  num_epochs = int(space['epochs'])\n","  precK_ls = []\n","  losses, val_losses = [],[]\n","\n","  for train_index, val_index in kfold.split(dataset['input_ids']):\n","\n","    BERT = TFBertForMaskedLM.from_pretrained(path_names['BERT_pretrained_model'],\n","                                             output_attentions=True)\n","    optim = tf.keras.optimizers.Adam(lr)\n","    losses = []\n","    correct_train = 0\n","    correct_val=0\n","    train_basket, val_basket = {},{}\n","\n","    train_basket['input_ids'] = tf.gather(dataset['input_ids'], indices=tf.constant(train_index))\n","    train_basket['attention_mask'] = tf.gather(dataset['attention_mask'], indices=tf.constant(train_index))\n","    train_basket['labels'] = tf.gather(dataset['labels'], indices=tf.constant(train_index))\n","    train_basket['missing_items'] = tf.gather(dataset['missing_items'], indices=tf.constant(train_index))\n","    train_basket['masked_index'] = tf.gather(dataset['masked_index'], indices=tf.constant(train_index))\n","\n","    val_basket['input_ids'] = tf.gather(dataset['input_ids'], indices=tf.constant(val_index))\n","    val_basket['attention_mask'] = tf.gather(dataset['attention_mask'], indices=tf.constant(val_index))\n","    val_basket['labels'] = tf.gather(dataset['labels'], indices=tf.constant(val_index))\n","    val_basket['missing_items'] = tf.gather(dataset['missing_items'], indices=tf.constant(val_index))\n","    val_basket['masked_index'] = tf.gather(dataset['masked_index'], indices=tf.constant(val_index))\n","\n","    #-----------------------------------------------------------------------------\n","    # ~~~ Training ~~~\n","    #-----------------------------------------------------------------------------\n","\n","    for epoch in range(num_epochs):\n","      correct_train = 0\n","      correct_val=0\n","      avg_loss = 0\n","      losses, val_losses = [],[]\n","      loader_train = tf.data.Dataset.from_tensor_slices((train_basket['input_ids'],\n","                                                        train_basket['attention_mask'],\n","                                                        train_basket['labels'],\n","                                                        train_basket['missing_items'],\n","                                                        train_basket['masked_index']\n","                                                        )).batch(params['batch_size'],drop_remainder=True) \n","                                                                           \n","      loop_train = notebook.tqdm(loader_train, position=0, leave=True, colour='green')\n","      for k, batch in enumerate(loop_train):\n","        if k==0:\n","            train_step_fn = tf.function(train_step).get_concrete_function(batch,optim, BERT)\n","\n","        loss, attentions = train_step_fn(batch)\n","\n","        losses.append(loss.numpy())\n","        avg_loss = sum(losses)/k\n","            \n","        loop_train.set_postfix(loss=avg_loss, data=\"train\", epochs=num_epochs, lr=lr)\n","\n","      #-----------------------------------------------------------------------------\n","      # ~~~ Validation ~~~\n","      #-----------------------------------------------------------------------------\n","      loader_val = tf.data.Dataset.from_tensor_slices((val_basket['input_ids'],\n","                                              val_basket['attention_mask'],\n","                                              val_basket['labels'],\n","                                              val_basket['missing_items'],\n","                                              val_basket['masked_index']\n","                                              )).batch(params['batch_size'],drop_remainder=True) \n","\n","      loop_val = notebook.tqdm(loader_val, position=0, leave=True, colour='yellow')\n","      correct_val = 0\n","\n","      for v, batch_val in enumerate(loop_val):\n","\n","        batch_size=params['batch_size']\n","        input_ids = batch_val[0]\n","        attention_mask = batch_val[1]\n","        labels = batch_val[2]\n","        missing_items = batch_val[3]\n","        masked_index = batch_val[4]\n","\n","        outputBERT_logits = BERT.call(input_ids, attention_mask=attention_mask, labels=labels,training=False).logits \n","        logits = []\n","\n","        for m, mask_ind in enumerate(masked_index.numpy()):\n","          lgts = outputBERT_logits[m,mask_ind,:]\n","          logits.append(lgts)\n","\n","        outputs_val_softmax = tf.nn.softmax(tf.convert_to_tensor(logits))\n","\n","        top_k_items = tf.squeeze(tf.nn.top_k(outputs_val_softmax,k=params['n_seqs'],sorted=False)[1])\n","\n","        for i in range(0,batch_size):\n","            for j in range(0, params['n_seqs']):\n","              if (top_k_items[i][j] == missing_items[i]):\n","                correct_val+=1\n","                break\n","\n","        precK_val = correct_val/((v+1)*batch_size)\n","\n","        loop_val.set_postfix(PrecisionAtK =precK_val, Correct=correct_val,  data=\"validation\")\n","\n","        precK_ls.append(precK_val)\n","\n","  precK_avg = np.mean(np.array(precK_ls))\n","  \n","  return {'loss': -precK_avg, 'lossG': losses, 'status': STATUS_OK}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k0CpUPTnlAAh"},"outputs":[],"source":["# Run Bayesian Optimization\n","best_params, loss = dict(), dict()\n","trials = Trials()\n","best_params = fmin(fn=BERT_Base_finetuning,\n","          space=bayes_parameters,\n","          algo=tpe.suggest,\n","          trials=trials,\n","          return_argmin=False, \n","          max_evals=15,\n","          rstate= np.random.RandomState(RANDOM_STATE))\n","\n","precisionk  = [-x['result']['loss']  for x in trials.trials] \n","losses  = [x['result']['loss']  for x in trials.trials]\n","best_idx = np.argmax(precisionk)\n","best_model = [x['result']['model']  for x in trials.trials][best_idx]"]},{"cell_type":"markdown","source":["Saving hyperparameter optimisation results"],"metadata":{"id":"48pgOgqOw6Q2"}},{"cell_type":"code","source":["import pickle\n","# Dump results to pickle files\n","with open(path_names['results_preck'], \"wb\") as fp:   #Pickling\n","    pickle.dump(precisionk, fp)\n","\n","with open(path_names['results_hyperparams'], \"wb\") as fp:   #Pickling\n","    pickle.dump(best_params, fp)    "],"metadata":{"id":"AIWv_Fq0H3Xf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Train Full**"],"metadata":{"id":"zpJ-O541IGBx"}},{"cell_type":"markdown","source":["Open hyperparameter optimization results"],"metadata":{"id":"YMV3qON7xARw"}},{"cell_type":"code","source":["# Open pickle files with results for testing\n","import pickle\n","with open(path_names['results_preck'], \"rb\") as fp:   # Unpickling\n","    precK_load = pickle.load(fp)\n","\n","with open(path_names['results_hyperparams'], \"rb\") as fp:   #Pickling\n","    best_params_load = pickle.load(fp)    "],"metadata":{"id":"hDz8HO9YH7WW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BERT = TFBertForMaskedLM.from_pretrained(path_names['BERT_pretrained_model'])\n","lr=best_params_load['lr']\n","num_epochs = best_params_load['epochs']\n","optim = tf.keras.optimizers.Adam(lr,)\n","losses = []\n","correct_train = 0\n","\n","#-----------------------------------------------------------------------------\n","# ~~~ Training ~~~\n","#-----------------------------------------------------------------------------\n","\n","for epoch in range(num_epochs):\n","  correct_train = 0\n","  correct_val=0\n","  avg_loss = 0\n","  losses, val_losses = [],[]\n","  loader_train = tf.data.Dataset.from_tensor_slices((dataset['input_ids'],\n","                                                    dataset['attention_mask'],\n","                                                    dataset['labels'],\n","                                                    dataset['missing_items'],\n","                                                    dataset['masked_index']\n","                                                    )).batch(params['batch_size'],drop_remainder=True) \n","                                                                        \n","  loop_train = notebook.tqdm(loader_train, position=0, leave=True, colour='green')\n","  for k, batch in enumerate(loop_train):\n","    if k==0:\n","        train_step_fn = tf.function(train_step).get_concrete_function(batch,optim, BERT)\n","\n","    input_ids = batch[0]\n","    attention_mask = batch[1]\n","    labels = batch[2]\n","    missing_items = batch[3]\n","    masked_index = batch[4]\n","\n","    with tf.GradientTape() as tape:\n","      res = BERT.call(input_ids, attention_mask=attention_mask, labels=labels, training=True)\n","      loss=tf.math.reduce_mean(res.loss)\n","\n","    optim.minimize(loss, var_list = BERT.trainable_variables, tape=tape)\n","    outputBERT_logits = res.logits \n","\n","    logits = []\n","    for m, mask_ind in enumerate(masked_index.numpy()):\n","      lgts = outputBERT_logits[m,mask_ind,:]\n","      logits.append(lgts)\n","    outputs_val_softmax = tf.nn.softmax(tf.convert_to_tensor(logits))\n","\n","    top_k_items = tf.squeeze(tf.nn.top_k(outputs_val_softmax,k=params['n_seqs'],sorted=False)[1])\n","\n","    for i in range(0,params['batch_size']):\n","      for j in range(0, params['n_seqs']):\n","        if (top_k_items[i][j] == missing_items[i]):\n","          correct_train+=1\n","          break\n","\n","    precK_train = correct_train/((k+1)*params['batch_size'])\n","\n","    losses.append(loss.numpy())\n","    avg_loss = sum(losses)/k\n","\n","    loop_train.set_postfix(loss=avg_loss,precK_train = precK_train, data=\"train\", epochs=num_epochs, lr=lr)"],"metadata":{"id":"ibdw2-vwIFfg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Test on test set**"],"metadata":{"id":"V3OM9WuRR6da"}},{"cell_type":"code","source":["for epoch in range(num_epochs):\n","    correct_test=0\n","    avg_loss = 0\n","    pr_ls, precK_ls=[],[]\n","    \n","    #-----------------------------------------------------------------------------\n","    # ~~~ Test ~~~\n","    #-----------------------------------------------------------------------------\n","    loader_test = tf.data.Dataset.from_tensor_slices((dataset_test['input_ids'],\n","                                              dataset_test['attention_mask'],\n","                                              dataset_test['labels'],\n","                                              dataset_test['missing_items'],\n","                                              dataset_test['masked_index']\n","                                              )).batch(params['batch_size'],drop_remainder=True) \n","\n","    loop_test = notebook.tqdm(loader_test, position=0, leave=True, colour='yellow')\n","    correct_test =0\n","    for t, batch_test in enumerate(loop_test):\n","\n","      batch_size=params['batch_size']\n","      input_ids = batch_test[0]\n","      attention_mask = batch_test[1]\n","      labels = batch_test[2]\n","      missing_items = batch_test[3]\n","      masked_index = batch_test[4]\n","\n","      outputBERT_logits = BERT.call(input_ids, attention_mask=attention_mask, labels=labels,training=False).logits \n","      logits = []\n","\n","      for m, mask_ind in enumerate(masked_index.numpy()):\n","        lgts = outputBERT_logits[m,mask_ind,:]\n","        logits.append(lgts)\n","      outputs_test_softmax = tf.nn.softmax(tf.convert_to_tensor(logits))\n","\n","      top_k_items = tf.squeeze(tf.nn.top_k(outputs_test_softmax,k=params['n_seqs'],sorted=False)[1])\n","\n","      for i in range(0,batch_size):\n","          pr = tf.math.reduce_sum(tf.where(tf.math.less(outputs_test_softmax[i,:], \n","                                                  outputs_test_softmax[i,missing_items[i].numpy()[0]]), 1.0, 0.0)).numpy()/tokenizer.vocab_size\n","          pr_ls.append(pr)\n","          for j in range(0, params['n_seqs']):\n","            if (top_k_items[i][j] == missing_items[i]):\n","              correct_test+=1\n","              break\n","\n","      mpr_test = np.array(pr_ls).sum()/((t+1)*batch_size) \n","      precK_test = correct_test/((t+1)*batch_size)\n","      \n","      loop_test.set_postfix(PrecisionAtK =precK_test, MPR=mpr_test, Correct=correct_test,  data=\"test\")"],"metadata":{"id":"jxladdprR51V"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"TEST TF - BERT base case REDDING.ipynb","provenance":[{"file_id":"1A3wTzYjqoZVuXfxNfdMTWjG3RAI8SvmW","timestamp":1643629509854},{"file_id":"15B9QJI6737fluhnsyoxkPnkW6Uoh459F","timestamp":1643117758680},{"file_id":"1oRfZnZhyObmIx0d_p7-AK0W5IM23lEsZ","timestamp":1642425862899},{"file_id":"1vZMOsayTPkcQ7Y97gJwwQKHqKr4_3BSn","timestamp":1642076413386}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}